# <-- [æ ¸å¿ƒ] å°è£…è®­ç»ƒã€éªŒè¯å¾ªçŽ¯å’ŒæŒ‡æ ‡è®¡ç®—
import torch
import torch.nn.functional as F
from tqdm import tqdm
import wandb
import logging
from pathlib import Path

class Trainer:
    """
    è®­ç»ƒå™¨ç±»ï¼Œå°è£…äº†æ‰€æœ‰ä¸Žæ¨¡åž‹è®­ç»ƒå’Œè¯„ä¼°ç›¸å…³çš„é€»è¾‘ã€‚
    è¿™ä½¿å¾—ä¸»è®­ç»ƒè„šæœ¬å¯ä»¥ä¿æŒç®€æ´ï¼Œåªè´Ÿè´£å¯¹è±¡çš„åˆå§‹åŒ–å’Œå¯åŠ¨è®­ç»ƒæµç¨‹ã€‚
    """
    def __init__(self, model, optimizer, train_loader, val_loader, config):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨ã€‚
        
        :param model: è¦è®­ç»ƒçš„PyTorchæ¨¡åž‹ã€‚
        :param optimizer: ç”¨äºŽæ›´æ–°æ¨¡åž‹å‚æ•°çš„ä¼˜åŒ–å™¨ã€‚
        :param train_loader: è®­ç»ƒæ•°æ®çš„DataLoaderã€‚
        :param val_loader: éªŒè¯æ•°æ®çš„DataLoaderã€‚
        :param config: åŒ…å«æ‰€æœ‰è¶…å‚æ•°å’Œè®¾ç½®çš„é…ç½®å¯¹è±¡ (Box)ã€‚
        """
        self.model = model
        self.optimizer = optimizer
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.config = config
        # è‡ªåŠ¨æ£€æµ‹å¹¶é€‰æ‹©å¯ç”¨çš„è®¾å¤‡ï¼ˆä¼˜å…ˆä½¿ç”¨GPUï¼‰
        self.device = torch.device(config.training.device if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        logging.info(f"âœ… æ¨¡åž‹å’Œæ•°æ®å°†ä½¿ç”¨è®¾å¤‡: {self.device}")

        # åˆå§‹åŒ– Weights & Biases (W&B) ç”¨äºŽå®žéªŒè·Ÿè¸ª
        # `project_name` å’Œ `run_name` ç”¨äºŽåœ¨W&Bä»ªè¡¨æ¿ä¸Šç»„ç»‡å®žéªŒ
        wandb.init(project=config.wandb.project_name, name=config.wandb.run_name, config=config)
        # `wandb.watch` ä¼šè‡ªåŠ¨è®°å½•æ¨¡åž‹çš„æ¢¯åº¦å’Œå‚æ•°ï¼Œä¾¿äºŽè°ƒè¯•å’Œåˆ†æž
        wandb.watch(self.model, log_freq=100)

    def _calculate_loss(self, predictions, labels):
        """
        è®¡ç®—ç”¨äºŽé¢„è®­ç»ƒä»»åŠ¡çš„æ··åˆæŸå¤±å‡½æ•°ã€‚
        ç”±äºŽæˆ‘ä»¬çš„å³°å‘é‡åŒ…å«è¿žç»­æ•°å€¼å’Œç¦»æ•£ç±»åˆ«ï¼Œå› æ­¤éœ€è¦ç»„åˆä¸åŒçš„æŸå¤±å‡½æ•°ã€‚

        :param predictions: æ¨¡åž‹çš„è¾“å‡ºï¼Œå½¢çŠ¶ä¸º [B, L, D]ã€‚
        :param labels: çœŸå®žçš„æ ‡ç­¾ï¼Œå½¢çŠ¶ä¸º [B, L, D]ï¼Œæœªè¢«æŽ©ç çš„ä½ç½®å¡«å……äº†-100ã€‚
        :return: è®¡ç®—å¾—åˆ°çš„æ ‡é‡æŸå¤±å€¼ã€‚
        """
        # 1. æ‰¾åˆ°è¢«æŽ©ç çš„ä½ç½® (å³æ ‡ç­¾ä¸­ä¸ä¸º-100çš„åœ°æ–¹)
        # æˆ‘ä»¬åªå…³å¿ƒæ¨¡åž‹åœ¨è¿™äº›è¢«æŽ©ç ä½ç½®ä¸Šçš„é¢„æµ‹æ€§èƒ½ã€‚
        # è¿™é‡Œæˆ‘ä»¬å‡è®¾ï¼Œå¦‚æžœä¸€ä¸ªå³°å‘é‡çš„ç¬¬ä¸€ä¸ªå…ƒç´ ä¸æ˜¯-100ï¼Œé‚£ä¹ˆæ•´ä¸ªå‘é‡éƒ½æ˜¯æœ‰æ•ˆæ ‡ç­¾ã€‚
        # è¿™ä¼šç”Ÿæˆä¸€ä¸ªå½¢çŠ¶ä¸º [B, L] çš„å¸ƒå°”æŽ©ç ã€‚
        active_mask = labels[:, :, 0] != -100
        
        # 2. ä½¿ç”¨ä¸Šé¢ç”Ÿæˆçš„æŽ©ç ï¼Œä»Žé¢„æµ‹å’Œæ ‡ç­¾ä¸­åªæŠ½å–å‡ºè¢«æŽ©ç ä½ç½®çš„å‘é‡ã€‚
        # è¿™æ ·ï¼ŒæŸå¤±å°†åªåœ¨è¿™äº›å­é›†ä¸Šè®¡ç®—ã€‚
        active_preds = predictions[active_mask]
        active_labels = labels[active_mask]

        # å¦‚æžœå½“å‰æ‰¹æ¬¡ä¸­æ²¡æœ‰ä»»ä½•è¢«æŽ©ç çš„æ ·æœ¬ï¼Œç›´æŽ¥è¿”å›ž0æŸå¤±ã€‚
        if active_preds.shape[0] == 0:
            return torch.tensor(0.0, device=self.device, requires_grad=True)

        # 3. ä»Žé…ç½®æ–‡ä»¶ä¸­èŽ·å–ç‰¹å¾å‘é‡ä¸åŒéƒ¨åˆ†çš„åˆ‡ç‰‡ç´¢å¼•
        s_mod, e_mod = self.config.feature_slices.modality
        s_num, e_num = self.config.feature_slices.numerical
        s_mult, e_mult = self.config.feature_slices.multiplicity

        # 4. åˆ†åˆ«è®¡ç®—ä¸åŒéƒ¨åˆ†çš„æŸå¤±
        # a) å¯¹äºŽè¿žç»­æ•°å€¼éƒ¨åˆ†ï¼ˆå¦‚å¼ºåº¦ã€m/zï¼‰ï¼Œä½¿ç”¨å‡æ–¹è¯¯å·®æŸå¤± (MSE)
        loss_mse = F.mse_loss(active_preds[:, s_num:e_num], active_labels[:, s_num:e_num])

        # b) å¯¹äºŽç¦»æ•£ç±»åˆ«éƒ¨åˆ†ï¼ˆå¦‚æ¨¡æ€ã€å¤šé‡æ€§ï¼‰ï¼Œä½¿ç”¨äº¤å‰ç†µæŸå¤± (Cross-Entropy)
        # æ¨¡åž‹çš„è¾“å‡ºæ˜¯logitsï¼Œè€Œæ ‡ç­¾æ˜¯one-hotç¼–ç ã€‚æˆ‘ä»¬éœ€è¦å…ˆç”¨argmaxä»Žæ ‡ç­¾ä¸­èŽ·å–ç±»åˆ«ç´¢å¼•ã€‚
        loss_ce_modality = F.cross_entropy(active_preds[:, s_mod:e_mod], torch.argmax(active_labels[:, s_mod:e_mod], dim=-1))
        loss_ce_mult = F.cross_entropy(active_preds[:, s_mult:e_mult], torch.argmax(active_labels[:, s_mult:e_mult], dim=-1))

        # 5. å°†å„éƒ¨åˆ†æŸå¤±åŠ æƒæ±‚å’Œï¼ˆå½“å‰æƒé‡å‡ä¸º1ï¼‰
        # æœªæ¥å¯ä»¥æ ¹æ®ä¸åŒä»»åŠ¡çš„é‡è¦æ€§è°ƒæ•´è¿™äº›æƒé‡ã€‚
        total_loss = loss_mse + loss_ce_modality + loss_ce_mult
        return total_loss

    def _run_epoch(self, dataloader, is_training=True):
        """
        è¿è¡Œä¸€ä¸ªå®Œæ•´çš„epochï¼Œå¯ä»¥æ˜¯è®­ç»ƒæˆ–éªŒè¯ã€‚

        :param dataloader: ç”¨äºŽè¯¥epochçš„æ•°æ®åŠ è½½å™¨ã€‚
        :param is_training: å¸ƒå°”å€¼ï¼Œå¦‚æžœä¸ºTrueï¼Œåˆ™æ‰§è¡Œè®­ç»ƒæ­¥éª¤ï¼ˆåå‘ä¼ æ’­å’Œä¼˜åŒ–ï¼‰ã€‚
        :return: è¯¥epochçš„å¹³å‡æŸå¤±ã€‚
        """
        # æ ¹æ®æ˜¯è®­ç»ƒè¿˜æ˜¯è¯„ä¼°ï¼Œè®¾ç½®æ¨¡åž‹çš„æ¨¡å¼
        self.model.train(is_training)
        total_loss = 0.0
        
        # ä½¿ç”¨tqdmåˆ›å»ºè¿›åº¦æ¡ï¼Œæ–¹ä¾¿ç›‘æŽ§
        desc = "Train" if is_training else "Eval"
        progress_bar = tqdm(dataloader, desc=desc)
        
        for batch in progress_bar:
            # å°†æ‰¹æ¬¡ä¸­çš„æ‰€æœ‰å¼ é‡ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
            masked_inputs, masks, labels = [b.to(self.device) for b in batch]

            # åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ï¼Œæ¯ä¸ªæ‰¹æ¬¡å‰æ¸…ç©ºä¹‹å‰çš„æ¢¯åº¦
            if is_training:
                self.optimizer.zero_grad()
            
            # ä½¿ç”¨torch.set_grad_enabledä¸Šä¸‹æ–‡ç®¡ç†å™¨æ¥æŽ§åˆ¶æ˜¯å¦è®¡ç®—æ¢¯åº¦
            # åœ¨éªŒè¯æ—¶å…³é—­æ¢¯åº¦è®¡ç®—å¯ä»¥èŠ‚çœå†…å­˜å¹¶åŠ é€Ÿè®¡ç®—
            with torch.set_grad_enabled(is_training):
                predictions = self.model(masked_inputs, masks)
                loss = self._calculate_loss(predictions, labels)

            # å¦‚æžœæ˜¯è®­ç»ƒæ¨¡å¼ï¼Œæ‰§è¡Œåå‘ä¼ æ’­å’Œä¼˜åŒ–å™¨æ­¥éª¤
            if is_training:
                loss.backward()
                self.optimizer.step()
            
            total_loss += loss.item()
            # åœ¨è¿›åº¦æ¡ä¸Šå®žæ—¶æ˜¾ç¤ºå½“å‰æ‰¹æ¬¡çš„æŸå¤±
            progress_bar.set_postfix(loss=loss.item())

        # è¿”å›žæ•´ä¸ªepochçš„å¹³å‡æŸå¤±
        return total_loss / len(dataloader)

    def _save_checkpoint(self, epoch, val_loss, is_best):
        """
        ä¿å­˜æ¨¡åž‹æ£€æŸ¥ç‚¹ã€‚åªåœ¨éªŒè¯æŸå¤±æ”¹å–„æ—¶ä¿å­˜â€œæœ€ä½³æ¨¡åž‹â€ã€‚

        :param epoch: å½“å‰çš„epochå·ã€‚
        :param val_loss: å½“å‰çš„éªŒè¯æŸå¤±ã€‚
        :param is_best: å¸ƒå°”å€¼ï¼ŒæŒ‡ç¤ºå½“å‰æ¨¡åž‹æ˜¯å¦æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¥½çš„ã€‚
        """
        # å¦‚æžœå½“å‰æ¨¡åž‹ä¸æ˜¯æœ€å¥½çš„ï¼Œåˆ™ä¸æ‰§è¡Œä»»ä½•æ“ä½œ
        if not is_best:
            return

        # ç¡®ä¿æ£€æŸ¥ç‚¹ç›®å½•å­˜åœ¨
        checkpoint_dir = Path(self.config.training.checkpoint_dir)
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        checkpoint_path = checkpoint_dir / "best_model.pt"
        
        # ä¿å­˜æ¨¡åž‹çŠ¶æ€å­—å…¸ã€ä¼˜åŒ–å™¨çŠ¶æ€ã€epochå·å’ŒéªŒè¯æŸå¤±
        torch.save({
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'val_loss': val_loss,
        }, checkpoint_path)
        logging.info(f"ðŸš€ æ–°çš„æœ€ä½³æ¨¡åž‹å·²ä¿å­˜ (Epoch {epoch+1}, Val Loss: {val_loss:.4f}) è‡³: {checkpoint_path}")

    def train(self):
        """
        æ‰§è¡Œå®Œæ•´çš„æ¨¡åž‹è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬å¤šä¸ªepochçš„è®­ç»ƒå’ŒéªŒè¯ã€‚
        """
        # åˆå§‹åŒ–æœ€ä½³éªŒè¯æŸå¤±ä¸ºä¸€ä¸ªæžå¤§å€¼
        best_val_loss = float('inf')
        
        for epoch in range(self.config.training.epochs):
            # è¿è¡Œä¸€ä¸ªè®­ç»ƒepoch
            train_loss = self._run_epoch(self.train_loader, is_training=True)
            # è¿è¡Œä¸€ä¸ªéªŒè¯epoch
            val_loss = self._run_epoch(self.val_loader, is_training=False)
            
            # è®°å½•æ—¥å¿—
            logging.info(f"Epoch {epoch+1}/{self.config.training.epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")
            # å°†æŒ‡æ ‡è®°å½•åˆ°W&B
            wandb.log({"epoch": epoch, "train_loss": train_loss, "val_loss": val_loss})

            # æ£€æŸ¥å½“å‰éªŒè¯æŸå¤±æ˜¯å¦æ˜¯åŽ†å²æœ€ä½³
            is_best = val_loss < best_val_loss
            if is_best:
                best_val_loss = val_loss
            
            # æ ¹æ®is_bestæ ‡å¿—å†³å®šæ˜¯å¦ä¿å­˜æ¨¡åž‹
            self._save_checkpoint(epoch, val_loss, is_best)